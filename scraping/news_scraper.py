from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from urllib.parse import urljoin

from bs4 import BeautifulSoup
import time
import requests
import json

def get_last_page_number(soup):
    # ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    navi = soup.select("table.Nnavi td a")
    page_numbers = []
    for a in navi:
        if 'page=' in a['href']:
            page = int(a['href'].split('page=')[-1])
            page_numbers.append(page)
    return max(page_numbers) if page_numbers else 1
    # return 1


def get_news_content(url):
    # ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=5)
        if res.status_code != 200:
            return ""
        soup = BeautifulSoup(res.text, "html.parser")

        # ìƒˆ êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„° ì‚¬ìš©
        content_area = soup.select_one("div#newsct_article article#dic_area")
        if not content_area:
            return ""
        return content_area.get_text(strip=True, separator="\n")
    except Exception as e:
        print(f"[Error] {url} - {e}")
        return ""

def get_news_from_page(soup):
    news_results = []
    bottom_section = soup.select("li.newsList")

    seen = set()
    for li in bottom_section:
        for a in li.select("a[title]"):
            href = a.get("href")
            if href and href not in seen:
                seen.add(href)
                # ì ˆëŒ€ URLì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ìƒëŒ€ URLì´ë©´ finance.naver.comì„ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                if href.startswith("http"):
                    full_url = href
                else:
                    full_url = urljoin("https://finance.naver.com", href)
                content = get_news_content(full_url)
                news_results.append({
                    "title": a["title"],
                    "link": full_url,
                    "section": "bottom",
                    "content": content  # ë³¸ë¬¸ ì¶”ê°€

                })
    return news_results


def get_news_by_date(date_str):
    base_url = f"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date={date_str}&page="

    # ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •
    options = Options()
    options.add_argument("--headless")
    service = Service("/usr/local/bin/chromedriver")  # ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ ì¡°ì •
    driver = webdriver.Chrome(service=service, options=options)

    # 1í˜ì´ì§€ ë¨¼ì € ì—´ì–´ì„œ ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    driver.get(base_url + "1")
    time.sleep(1)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    last_page = get_last_page_number(soup)
    print(f"{date_str} - ì´ {last_page} í˜ì´ì§€ ìˆìŒ")

    all_news = []

    for page in range(1, last_page + 1):
        driver.get(base_url + str(page))
        time.sleep(1)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        page_news = get_news_from_page(soup)
        all_news.extend(page_news)
        print(f"{page}í˜ì´ì§€: {len(page_news)}ê°œ ìˆ˜ì§‘ë¨")

    driver.quit()
    return all_news

if __name__ == "__main__":
    news_list = get_news_by_date("20250610")
    print(f"\nì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    for news in news_list[:3]:  # ë¯¸ë¦¬ë³´ê¸°
        print(f"- {news['title']}\n  ğŸ‘‰ {news['link']}\n  ğŸ“° {news['content'][:100]}...\n")

    # âœ… JSON íŒŒì¼ë¡œ ì €ì¥
    with open("../data/raw_news.json", "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=2)
    print("ğŸ“ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ data/raw_news.json íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")